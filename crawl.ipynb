{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from time import sleep\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "# 最多爬取2500部电影\n",
    "# 每个电影爬取10条影评\n",
    "# user-agent\n",
    "user_agents = [\n",
    "    \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "    \"Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)\",\n",
    "    \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)\",\n",
    "    \"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)\",\n",
    "    \"Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0\",\n",
    "    \"Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5\",\n",
    "    \"Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20\",\n",
    "    \"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52\",\n",
    "]\n",
    "def get_headers():\n",
    "    headers = {\n",
    "        \"User-Agent\":\n",
    "            random.choice(user_agents),\n",
    "        \"Connection\":\n",
    "            \"keep-alive\",\n",
    "        \"Referer\":\n",
    "            \"https://www.douban.com\"  # 站内访问\n",
    "    }\n",
    "    return headers\n",
    "\n",
    "URL_SET = set()\n",
    "REVIEW_NUM = 10\n",
    "DATA_PATH = './data/'\n",
    "delay = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 请求网页封装\n",
    "def request_douban(url, headers = get_headers()):\n",
    "    while True:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print('被拦截了，休息一下')\n",
    "            sleep(1200)\n",
    "        else:\n",
    "            break\n",
    "    sleep(1)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抓取影评\n",
    "def crawl_reviews(url, num = REVIEW_NUM):\n",
    "    try:\n",
    "        # 抓取num条影评\n",
    "        review_list = []\n",
    "        for i in range(0, num, 20): # 每页20条\n",
    "            url_review = url + '?start=' + str(i)\n",
    "            response = request_douban(url_review)\n",
    "            soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "            # 抓取影评\n",
    "            review_links = soup.select('div.main.review-item > div.main-bd > h2 > a')\n",
    "            if len(review_links) == 0:\n",
    "                return review_list\n",
    "            for index, link in enumerate(review_links):\n",
    "                if index == num - i: # 说明已经抓取了num条影评\n",
    "                    break\n",
    "                url_report = link['href']\n",
    "                response = request_douban(url_report)\n",
    "                soup = BeautifulSoup(response.text, 'lxml')\n",
    "                # 处理头部信息\n",
    "                header = soup.select('div.main > header.main-hd')[0]\n",
    "                # 作者\n",
    "                author = header.select('a')[0]\n",
    "                author_name = author.text.strip()\n",
    "                # 时间\n",
    "                time_span = header.select('div.main-meta > span')[0]\n",
    "                time = time_span.text.strip()\n",
    "\n",
    "                # 作者评分\n",
    "                try:\n",
    "                    author_star_span = soup.select('span.main-title-hide')[0]\n",
    "                    author_star = author_star_span.text.strip()\n",
    "                except:\n",
    "                    author_star = ''\n",
    "\n",
    "                # 处理主体信息\n",
    "                main = soup.select('div.main-bd')[0]\n",
    "                # 影评\n",
    "                review = main.select('div > div.review-content.clearfix')[0]\n",
    "                review_text = '\\n'.join([r.text for r in review])\n",
    "            \n",
    "                # 处理底部信息\n",
    "                footer = main.select('div.main-panel-useful')[0]\n",
    "                # 有用数\n",
    "                useful = footer.select('button.btn.useful_count.j.a_show_login')[0]\n",
    "                useful_count = useful.text.replace('有用', '').strip()\n",
    "                # 没用数\n",
    "                useless = footer.select('button.btn.useless_count.j.a_show_login')[0]\n",
    "                useless_count = useless.text.replace('没用', '').strip()\n",
    "\n",
    "                # 综合信息\n",
    "                review = {\n",
    "                    '作者': author_name,\n",
    "                    '作者评分': author_star,\n",
    "                    '时间': time,\n",
    "                    '影评': review_text,\n",
    "                    '有用数': useful_count,\n",
    "                    '没用数': useless_count\n",
    "                }\n",
    "                review_list.append(review)\n",
    "\n",
    "                # 检查各部分信息\n",
    "                for key, value in review.items():\n",
    "                    if value == '':\n",
    "                        if key == '作者评分':\n",
    "                            continue\n",
    "                        print('读取影评时,', key, '为空')\n",
    "                        print(url_report)\n",
    "        return review_list\n",
    "    except Exception as e:\n",
    "        print('读取影评时出错:', e)\n",
    "        print(url_report)\n",
    "        return review_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 爬取一部电影，并返回相关电影的url（加入到URL_SET中）\n",
    "def crawl_movie(url):\n",
    "    response = request_douban(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 处理详情页\n",
    "    header = soup.select('#content > h1')[0]\n",
    "    # 爬电影名字\n",
    "    movie_name_span = header.select('h1 > span')[0]\n",
    "    movie_name = movie_name_span.text.strip()\n",
    "    # 爬电影年份\n",
    "    movie_year_span = header.select('h1 > span.year')[0] \n",
    "    movie_year = movie_year_span.text.strip('()')\n",
    "\n",
    "    # 爬电影封面\n",
    "    movie_cover = soup.select('#mainpic > a > img')[0]['src']\n",
    "    cover_url = movie_cover.strip()\n",
    "    # 爬评分\n",
    "    movie_star_rating = soup.select('div.rating_self.clearfix > strong.ll.rating_num')[0]\n",
    "    movie_star = movie_star_rating.text.strip()\n",
    "    # 导演\n",
    "    try: \n",
    "        movie_director_span = soup.select('#info > span > span.attrs')[0]\n",
    "        movie_director = movie_director_span.text.strip()\n",
    "    except:\n",
    "        movie_director = ''\n",
    "    # 爬演员\n",
    "    try:\n",
    "        movie_actor_span = soup.select('#info > span.actor > span.attrs')[0]\n",
    "        movie_actor = movie_actor_span.text.strip('/')\n",
    "    except:\n",
    "        movie_actor = ''\n",
    "    # 爬简介\n",
    "    intro_span_all0 = soup.select('#link-report-intra.indent > span.all.hidden')\n",
    "    intro_span_all1 = soup.select('#link-report-intra.indent > span')\n",
    "    try:\n",
    "        intro_span = intro_span_all1[0] if len(intro_span_all0) == 0 else intro_span_all0[0]\n",
    "        intro = intro_span.text.strip()\n",
    "    except:\n",
    "        intro = ''\n",
    "    # 爬影评\n",
    "    review_url = url + 'reviews'\n",
    "    reviews = crawl_reviews(review_url)\n",
    "\n",
    "    # 综合信息\n",
    "    movie_info = {\n",
    "        '电影名': movie_name,\n",
    "        '年份': movie_year,\n",
    "        '评分': movie_star,\n",
    "        '封面': cover_url,\n",
    "        '导演': movie_director,\n",
    "        '演员': movie_actor,\n",
    "        '简介': intro,\n",
    "    }\n",
    "    if '豆瓣' in intro:\n",
    "        print('简介中有豆瓣')\n",
    "        print(url)\n",
    "    for key, value in movie_info.items():\n",
    "        if value == '':\n",
    "            print(key, '为空')\n",
    "            print(url)\n",
    "\n",
    "    return (movie_info, reviews)\n",
    "# 爬取相关电影，加入到URL_SET中\n",
    "def crawl_related_movie(url): \n",
    "    response = request_douban(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # 相关电影\n",
    "    related_links = soup.select('div.recommendations-bd > dl > dt > a')\n",
    "    if len(related_links) != 10:\n",
    "        print('相关电影数目不为10')\n",
    "        print(url)\n",
    "    for link in related_links:\n",
    "        related_url = link['href'].rstrip('?from=subject-page')\n",
    "        if related_url not in URL_SET:\n",
    "            url_queue.append(related_url)\n",
    "            URL_SET.add(related_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''爬取豆瓣电影top250链接'''\n",
    "def top250_crawer():\n",
    "    url_list = []\n",
    "    for i in range(0, 250, 25):\n",
    "        # 生成url\n",
    "        url_str = \"https://movie.douban.com/top250?start={}\".format(i)\n",
    "        response = request_douban(url_str)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        movie_items = soup.find_all('div', class_='item')\n",
    "\n",
    "        for item in movie_items:\n",
    "            # 加入相关电影的超链接\n",
    "            a = item.find_all('a')\n",
    "            url_list.append(a[1]['href'])\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "250\n",
      "相关电影数目不为1089 缓冲区长度： 416 迭代次数： 373\n",
      "相关电影数目不为10333 缓冲区长度： 1137 迭代次数： 1196\n",
      "相关电影数目不为10362 缓冲区长度： 1144 迭代次数： 1218\n",
      "现在的电影数量： 2503 缓冲区长度： 1216 迭代次数： 1287\r"
     ]
    }
   ],
   "source": [
    "top_urls = top250_crawer()\n",
    "URL_SET = set(top_urls)\n",
    "# print(URL_SET)\n",
    "url_queue = top_urls\n",
    "print(len(URL_SET))\n",
    "print(len(url_queue))\n",
    "i = 0\n",
    "while len(URL_SET) < 2500 and len(url_queue) != 0:\n",
    "    url = url_queue.pop(0)\n",
    "    crawl_related_movie(url)\n",
    "    i += 1\n",
    "    print('现在的电影数量：', len(URL_SET), '缓冲区长度：', len(url_queue), '迭代次数：', i, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_header = ['电影url']\n",
    "with open(DATA_PATH + 'movie_urls.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=url_header)\n",
    "    writer.writeheader()\n",
    "    for url in URL_SET:\n",
    "        writer.writerow({url_header:url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_reviews_in_xml(f, review_list):\n",
    "    f.write('<reviews>\\n')\n",
    "    for review in review_list:\n",
    "        f.write('<review>\\n')\n",
    "        for key, value in review.items():\n",
    "            f.write('<' + key + '>')\n",
    "            f.write(value)\n",
    "            f.write('</' + key + '>\\n')\n",
    "        f.write('</review>\\n')\n",
    "    f.write('</reviews>\\n')\n",
    "def write_movie_in_xml(dic, review_list, index, doc_dir_path):\n",
    "    with open(doc_dir_path + '{}.xml'.format(index), 'w', encoding='utf-8') as f:\n",
    "        f.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "        f.write('<movie>\\n')\n",
    "        for key, value in dic.items():\n",
    "            f.write('<' + key + '>')\n",
    "            f.write(value)\n",
    "            f.write('</' + key + '>\\n')\n",
    "        write_reviews_in_xml(f, review_list)\n",
    "        f.write('</movie>\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2503\n"
     ]
    }
   ],
   "source": [
    "# 读取所有电影的url\n",
    "url_list = []\n",
    "with open(DATA_PATH + 'movie_urls.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        url = row['电影url']\n",
    "        url_list.append(url)\n",
    "print(len(url_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "演员 为空ing 2434 ...\n",
      "https://movie.douban.com/subject/1966355/\n",
      "演员 为空ing 2447 ...\n",
      "https://movie.douban.com/subject/2117844/\n",
      "中场休息10分钟\n",
      "演员 为空ing 2456 ...\n",
      "https://movie.douban.com/subject/3149040/\n",
      "被拦截了，休息一下2475 ...\n",
      "被拦截了，休息一下2477 ...\n",
      "演员 为空\n",
      "https://movie.douban.com/subject/10793719/\n",
      "被拦截了，休息一下2485 ...\n",
      "被拦截了，休息一下2486 ...\n",
      "简介 为空ing 2489 ...\n",
      "https://movie.douban.com/subject/26939903/\n",
      "被拦截了，休息一下2491 ...\n",
      "被拦截了，休息一下2497 ...\n",
      "中场休息10分钟\n",
      "被拦截了，休息一下2498 ...\n",
      "被拦截了，休息一下\n",
      "被拦截了，休息一下\n",
      "crawling 2502 ...\r"
     ]
    }
   ],
   "source": [
    "delta = 2399\n",
    "for index, url in enumerate(url_list[delta:]):\n",
    "    if (index + 1) % 50 == 0:   # 每爬50部电影休息10分钟，主动休息，防止被封\n",
    "        print('中场休息10分钟')\n",
    "        sleep(600)\n",
    "    #if (index + 1) % 300 == 0:\n",
    "        #print('先不爬了')\n",
    "        #break\n",
    "    index += delta\n",
    "    print('crawling', index, '...', end='\\r')\n",
    "    dic, reviews = crawl_movie(url)\n",
    "    write_movie_in_xml(dic, reviews, index, DATA_PATH + 'movies/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'电影名': '阳光下的罪恶 Evil Under the Sun',\n",
       " '年份': '1982',\n",
       " '评分': '8.3',\n",
       " '封面': 'https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2173899536.jpg',\n",
       " '导演': '盖伊·汉弥尔顿',\n",
       " '演员': '彼得·乌斯蒂诺夫 / 玛吉·史密斯 / 科林·布莱克利 / 黛安娜·里格 / 丹尼斯·奎利 / 埃米莉·霍恩 / 罗迪·麦克道尔 / 詹姆斯·梅森 / 尼古拉斯·克莱 / 简·伯金',\n",
       " '简介': '阿伦娜·马歇尔（黛安娜·里格 Diana Rigg 饰）是一个美艳不可方物的女明星，天性放浪的她从不掩饰对其他男性的喜爱，也十分享受被男性追求的感觉，从而彻底忽略了丈夫和养女的感受。帕特里克（尼古拉斯·克莱 Nicholas Clay 饰）是个血气方刚的英俊小伙，虽然已经有了体弱多病的妻子克里斯汀（简·伯金 Jane Birkin 饰），但他依然难以抵挡向他暗送秋波的阿伦娜的诱惑。今年，位于莱瑟库姆湾旁小岛上的度假旅馆“快乐罗杰”成为了展示罪恶的舞台，来这里渡假的所有游客，似乎都和阿伦娜有个多多少少的利益纠葛。哪里有罪恶，哪里就有波罗（彼得·乌斯蒂诺夫 Peter Ustinov 饰），这次也不例外，聪明的他将一切矛盾都看在眼底。\\n                                    \\n                                \\u3000\\u3000阿伦娜死了，这令众人毫不意外。尽管每个人都有作案的动机，但每个人又都有完美的不在场证明。究竟是谁杀了阿伦娜？其实凶手早早的，便露出了他狐狸尾巴。'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic, reviews = crawl_movie(url_list[1043])\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_movie_in_xml(dic, reviews, 90, './data/movies/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://movie.douban.com/subject/4930379/'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_list[931]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'request_douban' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Desktop\\My University\\3-\\ir\\final\\my_engine\\crawl.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Desktop/My%20University/3-/ir/final/my_engine/crawl.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m request_douban(url_list[\u001b[39m931\u001b[39m], headers \u001b[39m=\u001b[39m get_headers())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'request_douban' is not defined"
     ]
    }
   ],
   "source": [
    "request_douban(url_list[931], headers = get_headers())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
