{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 豆瓣爬虫\n",
    "全局变量，以及库的导入,交给pagerank.py来完成，这里只需要导入pagerank.py即可\n",
    "- 每个最多爬取2500部电影\n",
    "- 每个电影爬取10条影评"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从crawl_request中拿到相应的请求函数\n",
    "from crawl_request import *\n",
    "import html\n",
    "import pickle\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crawl_request.py中，将请求网页的函数封装起来，方便调整拦截后的休息时间\n",
    "``` python\n",
    "# 请求网页封装\n",
    "def request_douban(url, headers = get_headers()):\n",
    "    while True:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print('被拦截了，休息一下')\n",
    "            sleep(1200)\n",
    "        else:\n",
    "            break\n",
    "    sleep(1)\n",
    "    return response\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 影评爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抓取影评\n",
    "def crawl_reviews(url, num = REVIEW_NUM):\n",
    "    try:\n",
    "        # 抓取num条影评\n",
    "        review_list = []\n",
    "        for i in range(0, num, 20): # 每页20条\n",
    "            url_review = url + '?start=' + str(i)\n",
    "            response = request_douban(url_review)\n",
    "            soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "            # 抓取影评\n",
    "            review_links = soup.select('div.main.review-item > div.main-bd > h2 > a')\n",
    "            if len(review_links) == 0:\n",
    "                return review_list\n",
    "            for index, link in enumerate(review_links):\n",
    "                if index == num - i: # 说明已经抓取了num条影评\n",
    "                    break\n",
    "                url_report = link['href']\n",
    "                response = request_douban(url_report)\n",
    "                soup = BeautifulSoup(response.text, 'lxml')\n",
    "                # 标题\n",
    "                title = soup.select('div.article > h1')[0].text.strip()\n",
    "                # 处理头部信息\n",
    "                header = soup.select('div.main > header.main-hd')[0]\n",
    "                # 作者\n",
    "                author = header.select('a')[0]\n",
    "                author_name = author.text.strip()\n",
    "                # 时间\n",
    "                time_span = header.select('div.main-meta > span')[0]\n",
    "                time = time_span.text.strip()\n",
    "\n",
    "                # 作者评分\n",
    "                try:\n",
    "                    author_star_span = soup.select('span.main-title-hide')[0]\n",
    "                    author_star = author_star_span.text.strip()\n",
    "                except:\n",
    "                    author_star = ''\n",
    "\n",
    "                # 处理主体信息\n",
    "                main = soup.select('div.main-bd')[0]\n",
    "                # 影评\n",
    "                review = main.select('div > div.review-content.clearfix')[0]\n",
    "                review_text = '\\n'.join([r.text for r in review])\n",
    "            \n",
    "                # 处理底部信息\n",
    "                footer = main.select('div.main-panel-useful')[0]\n",
    "                # 有用数\n",
    "                useful = footer.select('button.btn.useful_count.j.a_show_login')[0]\n",
    "                useful_count = useful.text.replace('有用', '').strip()\n",
    "                # 没用数\n",
    "                useless = footer.select('button.btn.useless_count.j.a_show_login')[0]\n",
    "                useless_count = useless.text.replace('没用', '').strip()\n",
    "\n",
    "                # 综合信息\n",
    "                review = {\n",
    "                    '作者': author_name,\n",
    "                    '标题': title,\n",
    "                    '作者评分': author_star,\n",
    "                    '时间': time,\n",
    "                    '影评': review_text,\n",
    "                    '有用数': useful_count,\n",
    "                    '没用数': useless_count\n",
    "                }\n",
    "                review_list.append(review)\n",
    "\n",
    "                # 检查各部分信息\n",
    "                for key, value in review.items():\n",
    "                    if value == '':\n",
    "                        if key == '作者评分':\n",
    "                            continue\n",
    "                        print('读取影评时,', key, '为空')\n",
    "                        print(url_report)\n",
    "        return review_list\n",
    "    except Exception as e:\n",
    "        print('读取影评时出错:', e)\n",
    "        print(url_report)\n",
    "        return review_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 电影爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 爬取一部电影，并返回相关电影的url（加入到URL_SET中）\n",
    "def crawl_movie(url):\n",
    "    response = request_douban(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 处理详情页\n",
    "    header = soup.select('#content > h1')[0]\n",
    "    # 爬电影名字\n",
    "    movie_name_span = header.select('h1 > span')[0]\n",
    "    movie_name = movie_name_span.text.strip()\n",
    "    # 爬电影年份\n",
    "    movie_year_span = header.select('h1 > span.year')[0] \n",
    "    movie_year = movie_year_span.text.strip('()')\n",
    "\n",
    "    # 爬电影封面\n",
    "    movie_cover = soup.select('#mainpic > a > img')[0]['src']\n",
    "    cover_url = movie_cover.strip()\n",
    "    # 爬评分\n",
    "    movie_star_rating = soup.select('div.rating_self.clearfix > strong.ll.rating_num')[0]\n",
    "    movie_star = movie_star_rating.text.strip()\n",
    "    # 导演\n",
    "    try: \n",
    "        movie_director_span = soup.select('#info > span > span.attrs')[0]\n",
    "        movie_director = movie_director_span.text.strip()\n",
    "    except:\n",
    "        movie_director = ''\n",
    "    # 爬演员\n",
    "    try:\n",
    "        movie_actor_span = soup.select('#info > span.actor > span.attrs')[0]\n",
    "        movie_actor = movie_actor_span.text.strip('/')\n",
    "    except:\n",
    "        movie_actor = ''\n",
    "    # 爬简介\n",
    "    intro_span_all0 = soup.select('#link-report-intra.indent > span.all.hidden')\n",
    "    intro_span_all1 = soup.select('#link-report-intra.indent > span')\n",
    "    try:\n",
    "        intro_span = intro_span_all1[0] if len(intro_span_all0) == 0 else intro_span_all0[0]\n",
    "        intro = intro_span.text.strip()\n",
    "    except:\n",
    "        intro = ''\n",
    "    # 爬影评\n",
    "    review_url = url + 'reviews'\n",
    "    reviews = crawl_reviews(review_url)\n",
    "\n",
    "    # 综合信息\n",
    "    movie_info = {\n",
    "        '电影名': movie_name,\n",
    "        '年份': movie_year,\n",
    "        '评分': movie_star,\n",
    "        '封面': cover_url,\n",
    "        '导演': movie_director,\n",
    "        '演员': movie_actor,\n",
    "        '简介': intro,\n",
    "    }\n",
    "    if '豆瓣' in intro:\n",
    "        print('简介中有豆瓣')\n",
    "        print(url)\n",
    "    for key, value in movie_info.items():\n",
    "        if value == '':\n",
    "            print(key, '为空')\n",
    "            print(url)\n",
    "\n",
    "    return (movie_info, reviews)\n",
    "# 爬取相关电影，加入到URL_SET中\n",
    "def crawl_related_movie(url): \n",
    "    response = request_douban(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # 相关电影\n",
    "    related_links = soup.select('div.recommendations-bd > dl > dt > a')\n",
    "    if len(related_links) != 10:\n",
    "        print('相关电影数目不为10')\n",
    "        print(url)\n",
    "    for link in related_links:\n",
    "        related_url = link['href'].rstrip('?from=subject-page')\n",
    "        if related_url not in URL_SET:\n",
    "            url_queue.append(related_url)\n",
    "            URL_SET.add(related_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬取电影"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''爬取豆瓣电影top250链接'''\n",
    "def top250_crawer():\n",
    "    url_list = []\n",
    "    for i in range(0, 250, 25):\n",
    "        # 生成url\n",
    "        url_str = \"https://movie.douban.com/top250?start={}\".format(i)\n",
    "        response = request_douban(url_str)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        movie_items = soup.find_all('div', class_='item')\n",
    "\n",
    "        for item in movie_items:\n",
    "            # 加入相关电影的超链接\n",
    "            a = item.find_all('a')\n",
    "            url_list.append(a[1]['href'])\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "250\n",
      "相关电影数目不为1089 缓冲区长度： 416 迭代次数： 373\n",
      "相关电影数目不为10333 缓冲区长度： 1137 迭代次数： 1196\n",
      "相关电影数目不为10362 缓冲区长度： 1144 迭代次数： 1218\n",
      "现在的电影数量： 2503 缓冲区长度： 1216 迭代次数： 1287\r"
     ]
    }
   ],
   "source": [
    "top_urls = top250_crawer()\n",
    "URL_SET = set(top_urls)\n",
    "# print(URL_SET)\n",
    "url_queue = top_urls\n",
    "print(len(URL_SET))\n",
    "print(len(url_queue))\n",
    "i = 0\n",
    "while len(URL_SET) < 2500 and len(url_queue) != 0:\n",
    "    url = url_queue.pop(0)\n",
    "    crawl_related_movie(url)\n",
    "    i += 1\n",
    "    print('现在的电影数量：', len(URL_SET), '缓冲区长度：', len(url_queue), '迭代次数：', i, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_header = ['电影url']\n",
    "with open(DATA_PATH + 'movie_urls.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=url_header)\n",
    "    writer.writeheader()\n",
    "    for url in URL_SET:\n",
    "        writer.writerow({url_header:url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_reviews_in_xml(f, review_list):\n",
    "    f.write('<reviews>\\n')\n",
    "    for review in review_list:\n",
    "        f.write('<review>\\n')\n",
    "        for key, value in review.items():\n",
    "            f.write('<' + key + '>')\n",
    "            value = html.escape(value)\n",
    "            f.write(value)\n",
    "            f.write('</' + key + '>\\n')\n",
    "        f.write('</review>\\n')\n",
    "    f.write('</reviews>\\n')\n",
    "def write_movie_in_xml(dic, review_list, index, doc_dir_path):\n",
    "    with open(doc_dir_path + '{}.xml'.format(index), 'w', encoding='utf-8') as f:\n",
    "        f.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "        f.write('<movie>\\n')\n",
    "        for key, value in dic.items():\n",
    "            f.write('<' + key + '>')\n",
    "            value = html.escape(value)\n",
    "            f.write(value)\n",
    "            f.write('</' + key + '>\\n')\n",
    "        write_reviews_in_xml(f, review_list)\n",
    "        f.write('</movie>\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2503\n"
     ]
    }
   ],
   "source": [
    "# 读取所有电影的url\n",
    "url_list = read_movie_url()\n",
    "# 爬取电影信息\n",
    "delta = 0\n",
    "for index, url in enumerate(url_list[delta:]):\n",
    "    if (index + 1) % 50 == 0:   # 每爬50部电影休息10分钟，主动休息，防止被封\n",
    "        print('中场休息10分钟')\n",
    "        sleep(600)\n",
    "    #if (index + 1) % 300 == 0:\n",
    "        #print('先不爬了')\n",
    "        #break\n",
    "    index += delta\n",
    "    print('crawling', index, '...', end='\\r')\n",
    "    dic, reviews = crawl_movie(url)\n",
    "    write_movie_in_xml(dic, reviews, index, DATA_PATH + 'test_movies/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬取影评链接\n",
    "\n",
    "下面爬取网站的链接，为链接分析做准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_review_link(url):\n",
    "    '''\n",
    "    爬取一个电影的所有影评链接，返回一个列表\n",
    "    '''\n",
    "    response = request_douban(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = []\n",
    "    try:\n",
    "        l = soup.select('div.main.review-item > div.main-bd > h2 > a')\n",
    "        for link in l:\n",
    "            links.append(link['href'])\n",
    "    except:\n",
    "        links = []\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_num(index):\n",
    "    '''\n",
    "    根据电影的index，返回影评数目\n",
    "    '''\n",
    "    # 解析XML文件\n",
    "    xml_path = DATA_PATH + 'movies/{}.xml'.format(index)\n",
    "    tree = ET.parse(xml_path, ET.XMLParser(encoding='utf-8'))\n",
    "    root = tree.getroot()\n",
    "\n",
    "    return len(root.findall('reviews/review'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_header = ['影评url']\n",
    "with open(DATA_PATH + 'review_urls.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=url_header)\n",
    "    writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中场休息10分钟 896 ...\n",
      "中场休息10分钟 996 ...\n",
      "中场休息10分钟 1096 ...\n",
      "中场休息10分钟 1196 ...\n",
      "中场休息10分钟 1296 ...\n",
      "中场休息10分钟 1396 ...\n",
      "中场休息10分钟 1496 ...\n",
      "中场休息10分钟 1596 ...\n",
      "中场休息10分钟 1696 ...\n",
      "中场休息10分钟 1796 ...\n",
      "中场休息10分钟 1896 ...\n",
      "中场休息10分钟 1996 ...\n",
      "中场休息10分钟 2096 ...\n",
      "中场休息10分钟 2196 ...\n",
      "中场休息10分钟 2296 ...\n",
      "被拦截了，休息一下2303 ...\n",
      "被拦截了，休息一下2304 ...\n",
      "中场休息10分钟 2396 ...\n",
      "中场休息10分钟 2496 ...\n",
      "被拦截了，休息一下2497 ...\n",
      "crawling 2502 ...\r"
     ]
    }
   ],
   "source": [
    "# 现在，通过遍历所有电影的url，爬取所有影评的url，写入到csv文件中\n",
    "# 读取所有电影的url\n",
    "url_header = ['影评url']\n",
    "url_list = read_movie_url()\n",
    "# 爬取影评\n",
    "delta = 798\n",
    "for index, url in enumerate(url_list[delta:]):\n",
    "    if (index + 1) % 100 == 0:   # 每爬100部电影休息10分钟，主动休息，防止被封\n",
    "        print('中场休息10分钟')\n",
    "        sleep(600)\n",
    "    index += delta\n",
    "    print('crawling', index, '...', end='\\r')\n",
    "    url = url + 'reviews'\n",
    "    review_links = crawl_review_link(url)\n",
    "    review_num = get_review_num(index)\n",
    "    with open(DATA_PATH + 'review_urls.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=url_header)\n",
    "        for i in range(review_num):\n",
    "            try:\n",
    "                writer.writerow({url_header[0]:review_links[i]})\n",
    "            except:\n",
    "                print('写入影评链接时出错：')\n",
    "                print('影评数目：', review_num, '影评链接数目：', len(review_links), 'index：', index)\n",
    "                print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://movie.douban.com/subject/1296147/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_list = read_movie_url()\n",
    "url_list[797]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬取链接关系\n",
    "\n",
    "爬取链接的from，to，以及链接的类型，首先爬取的是电影的链接，然后爬取影评的链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://movie.douban.com/top250\"\n",
    "\n",
    "url_header = ['from_url', 'to_url']\n",
    "with open(DATA_PATH + 'links.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=url_header)\n",
    "    writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = []\n",
    "s = {}\n",
    "total_len = 0\n",
    "q.append(url)\n",
    "while len(q) != 0 and total_len < 100000:\n",
    "    l = q.pop(0)\n",
    "    links = get_related_links(l)\n",
    "    s.add(l)\n",
    "    total_len += len(links)\n",
    "    with open(DATA_PATH + 'links.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=url_header)\n",
    "        for link in links:\n",
    "            writer.writerow({'from_url': url, 'to_url': link})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
