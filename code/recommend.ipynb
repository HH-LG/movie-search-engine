{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推荐系统"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正则化处理\n",
    "import re\n",
    "import string\n",
    "import jieba\n",
    "\n",
    "DATA_PATH = \"../data/\"\n",
    "\n",
    "class Normalization:\n",
    "    def __init__(self):\n",
    "        with open(DATA_PATH + 'stop_words.txt', 'r', encoding='utf-8') as f:\n",
    "            self.stopword_list = f.read().split('\\n')\n",
    "    \n",
    "    def tokenize_text(self, text):\n",
    "        tokens = jieba.lcut(text) # 分词\n",
    "        tokens = [token.strip() for token in tokens] # 去除空格\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def remove_special_characters(self, text):\n",
    "        tokens = self.tokenize_text(text)\n",
    "        pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "        filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
    "        filtered_text = ' '.join(filtered_tokens)\n",
    "        \n",
    "        return filtered_text\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        tokens = self.tokenize_text(text)\n",
    "        filtered_tokens = [token for token in tokens if token not in self.stopword_list]\n",
    "        filtered_text = ''.join(filtered_tokens)\n",
    "        \n",
    "        return filtered_text\n",
    "    \n",
    "    def normalize_corpus(self, corpus):\n",
    "        normalized_corpus = []\n",
    "        for text in corpus:\n",
    "            text = ' '.join(jieba.lcut(text))\n",
    "            normalized_corpus.append(text)\n",
    "        \n",
    "        return normalized_corpus\n",
    "\n",
    "normalization = Normalization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from database import *\n",
    "\n",
    "DATA_PATH = '../data/'\n",
    "\n",
    "with open(DATA_PATH + 'stop_words.txt', 'r', encoding='utf-8') as f:\n",
    "    stop_words = f.read().split('\\n')\n",
    "\n",
    "sql1 = \"select * from movies\"\n",
    "sql2 = \"select * from reviews\"\n",
    "\n",
    "# 电影数据\n",
    "cursor.execute(sql1)\n",
    "items = cursor.fetchall()\n",
    "lenMovie = len(items)\n",
    "data = [items[i][1] + items[i][7] for i in range(lenMovie)]\n",
    "\n",
    "\n",
    "# 评论数据\n",
    "cursor.execute(sql2)\n",
    "items = cursor.fetchall()\n",
    "lenReviews = len(items)\n",
    "data += [items[i][3] + items[i][7] for i in range(lenReviews)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalizing...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\24746\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15  1  5 ...  0  1  1]\n"
     ]
    }
   ],
   "source": [
    "# 标准化语料库\n",
    "print(\"normalizing...\")\n",
    "normData = normalization.normalize_corpus(data)\n",
    "\n",
    "print(\"training...\")\n",
    "# 使用 TF-IDF 将文本转换为向量\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "X = vectorizer.fit_transform(normData)\n",
    "\n",
    "# 使用 K-means 进行聚类\n",
    "kmeans = KMeans(n_clusters=20, random_state=0).fit(X)\n",
    "\n",
    "# 输出每个查询的聚类标签\n",
    "print(kmeans.labels_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([(15, 591), (1, 8693), (5, 926), (9, 364), (3, 312), (2, 240), (13, 1146), (0, 4029), (4, 1084), (18, 952), (6, 721), (7, 2620), (14, 366), (12, 398), (8, 506), (16, 379), (11, 1971), (10, 292), (19, 278), (17, 150)])\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "# 获取每个cluster的数量\n",
    "c = Counter(clusters)\n",
    "print(c.items())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(DATA_PATH + 'kmeans.pkl', 'wb') as f:\n",
    "    pickle.dump(kmeans, f)\n",
    "\n",
    "with open(DATA_PATH + 'vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调整数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql = \"alter table movies add cluster int(11)\"\n",
    "# cursor.execute(sql)\n",
    "# sql = \"alter table reviews add cluster int(11)\"\n",
    "# cursor.execute(sql)\n",
    "\n",
    "sql = \"update movies set cluster=%s where id=%s\"\n",
    "for i in range(lenMovie):\n",
    "    cursor.execute(sql, (clusters[i], i+1))\n",
    "\n",
    "sql = \"update reviews set cluster=%s where id=%s\"\n",
    "for i in range(lenReviews):\n",
    "    cursor.execute(sql, (clusters[i+lenMovie], i+1))\n",
    "\n",
    "db.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
